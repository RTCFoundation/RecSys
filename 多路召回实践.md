# 资讯推荐中多路召回实践

## 1. 召回服务框架介绍

![RecallFramework](/Users/menglingfeng/Documents/GitHub/RecSys/images/RecallFramework.jpg)

### 1.1 任务调度平台

这里主要介绍下比较主流的任务调度框架-airflow

airflow是一块开源的分布式任务调度框架，它将一个具有依赖关系的工作流，组装成一个有向无环图

特点：

1. 分布式任务调度：允许一个工作流的task在堕胎worker上同时执行
2. 可构建任务依赖：以有向无环图的方式构建任务依赖关系
3. task原子性：工作流上每个task都是原子可重试的，一个工作流某个环节的task失败，可自动或手动进行重试，不必从头开始任务

注：任务调度平台通常是大数据部或算法工程组的人联合开发，涉及数据、平台、算法、底层存储和监控等

![airflow](/Users/menglingfeng/Documents/GitHub/RecSys/images/airflow.jpg)

一个dag表示一个定时的工作流

示例：基于item_cf的任务调度

涉及任务

1. 抽取日志数据任务
2. item_cf计算任务（计算相似矩阵）

任务必须先抽取数据，后计算。因为存在先后依赖关系，所以必须设置这两个task依赖关系

### 1.2 向量服务平台

向量服务平台，也称为向量检索服务，其解决的问题是从海量向量数据中高精度、高性能地召回出与目标最相似的数据

向量服务平台的底层框架有

1. 基于量化的索引
2. 基于树的索引
3. 基于图的索引
4. 基于哈希的索引

这里主要介绍使用最多的基于树的索引

方法原理：搜索树思想，用超平面将高维空间分割成多个子空间，并把这些子空间以树型结构存储

算法实现：ANN搜索算法（faiss，Annoy，balltree）

![VectorService](/Users/menglingfeng/Documents/GitHub/RecSys/images/VectorService.jpg)

示例：

matrix_cf获取user_embedding和item_embedding

存入向量库中并加载到向量检索中提供服务



ANN算法-Balltree 构建原理

选择一个距离当前圆心最远的观测点i1和距离i1最远的观测点i2，将圆中所有离这两个点最近的观测点都赋给这两个簇的中心。然后计算每一个簇的中心点和包含所有其所属观测点的最小半径，不断递归

### 1.3 特征服务

特征服务实际上为一个存储用户特征和物料特征的存储平台

特点：

1. 存储的特征包含原始型和处理型
2. 存储的特征是实时更新的
3. 对外提供的服务是并发、高效、安全的
4. 分布式、可扩展性

示例：

用户点击历史特征

hist:[item2, item1]

当用户点击了item3，特征实时更新

Hist:[item3, item2, item1]

### 1.4 redis存储平台

redis存储平台是一个开源的、基于内存的数据结构存储器，用作数据库、缓存和消息中间件

特点：

1. C/S架构
2. 可分布式、可扩展性、高吞吐量

## 2. 资讯多路召回开发

### 2.1 特征工程

抽取特征并保存到redis数据库中

```python
# redis info
# db = 1 用户特征
# db = 2 文章特征
# db = 3 matrix_cf的文章相似矩阵
# db = 4 item_cf的文章相似矩阵
# db = 5 user_cf的用户相似矩阵
# db = 6 fm_i2i的文章相似矩阵
# db = 7 fm召回的文章隐向量
# db = 8 fm召回的用户特征隐向量
def save_redis(item, db = 1):
  redis_url = '127.0.0.1:6379/' + str(db)
  pool = redis.from_url(redis_url)
  try:
    for item in items:
      pool.set(item[0], item[1])
  except:
    traceback.print_exc()
```

```python
def get_item_feature():
  ds = pd.read_csv("./data/articles.csv")
  ds = ds.to_dict(orient = 'records')
  item_feature = []
  # (1336, {'article_id': 1336, 'category_id': 1, 'created_at_ts' : 1474330000, 'words_count' : 233})
  for d in ds:
    item_feature.append((d['article_id'], json.dumpes(d)))
  save_redis(item_feature, 2)
```

```python
def get_user_feature():
  ds = pd.read_csv("./data/click_log.csv")
  click_df = ds.sort_values('timestamp')
  #{100000:(4,13)}
  user_environment_region_dict = {}
  for info in zip(click_df['user_id'],
                 click_df['environment'], click_df['region']):
    user_environment_region_dict[info[0]] = (info[1], info[2])
    
  def make_item_time_pair(df):
    return list(zip(df['article_id'], df['timestamp']))
  
  # {100000, [(160417, 1507029570190), (5409, 1507029571478)]}
  user_item_time_df = click_df.groupby('user_id')[
    	'article_id', 'timestamp'].apply(
    	lambda x: make_item_time_pair(x)) \
  		.reset_index().rename(columns = {0: 'item_time_list'})
  
  user_item_time_dic = dict(
  	zip(user_item_time_df['user_id'], user_item_time_df['item_time_list']))
  
  user_feature = []
  for user, item_time_dict in user_item_time_dict.items():
    info = user_environment_region_dict[user]
    tmp = (str(user), json.dumps({
      'user_id': user, # 100000
      'hists': item_time_dict,
      'environment': info[0],
      'region': info[1]
    }))
    user_feature.append(tmp)
    
  save_redis(user_feature, 1)
```

### 2.2 Item_cf召回

ItemCF的主要思想：给用户推荐之前喜欢物品的相似物品

基于物品的协同过滤算法主要有三步：

1. 计算物品之间的相似度
2. 计算推荐结果
3. 惩罚热门物品/加入关联规则

物品相似度计算公式

wij  = 对物品i和物品j共同产生过行为的用户数/对物品i产生过行为的个数

可以对活跃用户和热门物品都进行惩罚

然后我们会基于关联规则对相似度权重进行加权

1. 考虑时间因素，两篇资讯点击时间相近权重大
2. 考虑资讯自身累呗因素，资讯类别相同，权重大

```python
# 根据点击时间获取用户的点击文章序列
# {user1:[(item1, time1), (itme2, time2),...], ...}
def get_user_item_time(click_df):
  click_df = click_df.sort_values('timestamp')
  def make_item_time_pair(df):
    return list(zip(df['article_id'], df['timestamp']))
  
  user_item_time_df = click_df.groupby('user_id')[
    'article_id', 'timestamp'].apply(
  	lambda x: make_itme_time_pair(x))\
  	.reset_index().rename(columns = {0: 'item_time_list'})
  
  user_item_time_dict = dict(
  	zip(user_time_time_df['user_id'], user_item_time_df['item_time_list']))
  return user_item_time_dict
```

```python
def item_cf_sim(user_item_time_dict, pool, cut_off = 20):
  # 定义一个缓存item信息的缓存区
  item_info = {}
  # 计算物品相似度
  i2i_sim = {}
  item_count = defaultdict(int)
  for user, item_time_list in tqdm(user_item_time_dict.item()):
    # 在基于商品的协同过滤优化的同时可以考虑时间因素
    for loc1, (i, i_click_time) in enumerate(item_time_list):
      item_cnt[i] += 1
      i2i_sim.setdefault(i, {})
      for loc2, (j, j_click_time) in enumerate(item_time_list):
        if i == j:
          continue
        # 点击时间权重，其中的参数可以调节。点击时间相近权重大
        click_time_weight = np.exp(
        0.7 ** np.abs(i_click_time - j_click_time))
        
        # 两篇文章的类别权重，其中类别相同权重越大
        item_i_info = item_info.get(i, None)
        if item_i_info is None:
          item_i_info = json.loads(pool.get(str(i)))
          item_info[i] = item_i_info
        item_j_info = item_info.get(j, None)
        if item_j_info is None:
          item_j_info = json.loads(pool.get(str(j)))
          item_info[j] = item_j_info
        type_weight = 1.0 if item_i_info['category_id'] == item_j_info['category_id'] else 0.7
        i2i_sim[i].setdefault(j, 0)
        
        # 考虑多种因素的权重计算最终的文章之间的相似度
        i2i_sim[i][j] += click_time_weight * type_wight/math.log(len(item_time_list) + 1)
  print("item_info get nums: ", len(item_info))
  i2i_sim_ = i2i_sim.copy()
  for i, related_items in i2i_sim.items():
    tmp = {}
    for j, wij in related_items.items():
      tmp[j] = wij / math.sqrt(item_cnt[i] * item_cnt[j])
    i2i_sim_[i] = sorted(
    	tmp.items(), key = lambda _: _[1], reverse = True)[:cut_off]
  # 将得到的相似性矩阵保存到redis
  save_redis(i2i_sim_, db = 4)
```

```python
# item_cf测试
click_df = pd.read_csv("./data/click_log.csv")
print("user history gen ...")
user_item_time_dict = get_user_item_time(click_df)

redis_url = "127.0.0.1:6379/2"
pool = redis.from_url(redis_url)
item_cf_sim(user_item_time_dict, pool. cut_off = 200)
```

### 2.3 user_cf召回

主要思想：给用户推荐与其相似的用户喜欢的物品，模式是u2u2i

用户协同过滤算法主要也有三步

1. 计算用户之间的相似度
2. 计算推荐结果
3. 惩罚热门物品/加入关联规则

```python
# 根据时间获取商品被点击的用户序列
# {item: [(user1, time1), (user2, time2), ...], ...}
def get_item_user_time_dict(click_df):
  def make_user_time_pair(df):
    return list(zip(df['user_id'], df['timestamp']))
  click_df = click_df.sort_values('timestamp')
  item_user_time_df = click_df.groupby('article_id')[
    'user_id', 'time_stamp'].apply(lambda x : make_user_time_pair(x)) \
  	.reset_index().rename(columns = {0: 'user_time_list'})
  item_user_time_dict = dict(zip(item_user_time_df['article_id'],
                                item_user_time_df['user_time_list']))
  return item_user_time_dict
```

```python
def user_cf_sim(item_user_time_dict, pool, cut_off = 20):
  # 定义一个缓存user信息的缓存区
  user_info = {}
  # 计算用户的相似度
  u2u_sim = {}
  user_cnt = defaultdict(int)
  for item, user_time_list in tqdm(item_user_time_dict.items()):
    for u, click_time in user_time_list:
      user_cnt[u] += 1
      u2u_sim.setdefault(u, {})
      for v, click_time in user_time_list:
        u2u_sim[u].setdefault(v, 0)
        if u == v:
          continue
          
        # 用户平均活跃度作为活跃度的权重，这里的公式可以改善
        # 两篇文章的类别权重，其中类别相同权重越大
        user_u_info = user_info.get(u, None)
        if user_u_info is None:
          user_u_info = json.loads(pool.get(str(u)))
          user_info[u] = user_u_info
        if user_v_info is None:
          user_v_info = json.loads(pool.get(str(v)))
          user_info[v] = user_v_info
        activate_weight = 0.1 * 0.5 * (len(user_v_info['hists']) +
                                      len(user_u_info['hists']))
        u2u_sim[u][v] += activate_weight / math.log(
        				len(user_time_list) + 1)
        
  u2u_sim_ = u2u_sim.copy()
  for u, related_users in u2u_sim.items():
    tmp = {}
    for v, wij in related_users.items():
      tmp[v] = wij / math.sqrt(user_cnt[u] * user_cnt[v])
    u2u_sim_[u] = sorted(
    	tmp.items(), key = lambda _: _[1], reverse = True)[:cut_off]
  
  # 将得到的相似性矩阵保存到本地
  save_redis(u2u_sim_, db = 5)  
```

```python
# 测试
click_df= pd.read_csv("./data/click_log.csv")
print('item history gen ...')
item_user_time_dict = get_time_user_time_dict(click_df)

redis_url = '127.0.0.1:6379/1'
pool = redis.from_url(redis_url)
user_ct_sim(item_user_time_dict, pool, cut_off = 20)
```

### 2.4 MatrixCF算法召回

```python
# 向量检索相似度计算
# top K值得的每个item，faiss搜索后返回最相似的top K个Item
def embedding_sim(item_emb_file, cut_off = 20):
  # 文章索引与文章id的字典映射
  item_embedding = read_embedding_file(item_emb_file)
  item_idx_2_rawid_dict= {}
  item_emb_np = []
  for i, (k, v) in enumerate(item_embedding.items()):
    item_idx_2_rawid_dic[i] = k
    item_emb_np.append(v)
  
  item_emb_np = np.asarray(item_emb_np)
  item_emb_np = item_emb_np /np.linalg.norm(
  	item_emb_np, axis = 1, keepdims = True)
  
  # 建立faiss/BallTree索引
  item_tree = neighbors.BallTree(item_emb_np, leaf_size = 40)
  # 相似度查询，给每个索引位置上的向量返回top K个item以及相似度
  sim, idx = item_tree.query(item_emb_np, cut_off) # 返回的是列表
  
  # 将向量检索的结果保存成原始id的对应关系
  item_emb_sim_dict = {}
  for target_idx, sim_value_list, rele_idx_list in tqdm(
  		zip(range(len(item_emb_np), sim, idx))):
    target_raw_id = item_idx_2_rawid_dict[target_idx]
    sim_tmp = {}
    # 从1开始是为了去掉商品本身，所以最终获得的相似商品只有top K-1
    for rele_idx, sim_value in zip(rele_idx_list[1:], sim_value_list[1:]):
      rele_raw_id = item_idx_2_rawid_dict[rele_idx]
      sim_tmp[rele_raw_id] = sim_value
    item_emb_sim_dict[target_raw_id] = sorted(
    	sim_tmp.items(), key = lambda _: _[1], reverse = True)[:cut_off]
  
  item_simi_tuple = [(_, json.dumps(v)) for _, v in item_emb_sim_dict.items()]
  save_redis(item_simi_tuple, db = 3)
```

### 2.5 FM算法召回

FM召回，英文全称是“Factorization Machine”，是一种基于矩阵分解的机器学习算法，是为了解决大规模稀疏矩阵中特征组合问题

基于FM召回算法有三步

1. 初始化线性部分LR的权重W和交叉部分的隐向量权重V
2. 基于计算公式，分别求出线性部分的输出和交叉部分的输出
3. 综合两种输出获得预估值，然后交叉熵loss，梯度更新权重W和V，最终获得每个特种的隐向量权重V

具体开发流程步骤

1. 抽取数据，构建正负样本
2. 基于FM模型对构建的数据进行训练
3. 将参数服务器中的对应参数变量保存
4. 基于对应参数向量获取到item向量（文章向量），计算item的相似矩阵存入reidis中
5. 把对应用户特征向量V存入对应的redis，用于向量服务

```python
# 对fm召回数据进行构建
def get_fm_recall_data(data_path, user_pool):
  ds = pd.read_csv(data_path)
  ds = ds[['user_id', "article_id"]]
  
  items = list(ds['article_id']).unique()
  
  ds = zip(ds['user_id'], ds['article_id'])
  data = []
  
  for u, i in ds:
    user_info = json.loads(user_pool.get(u))
    # 正样本， i表示文章id
    data.append(
    	(u, user_info['environment'], user_info['region'], i, 1))
    for t in random.sample(items, 10):
    # 负样本， t表示文章id，用户没有点击行为
      data.append(
      	(u, user_info['environment'], user_info["region"], t, 0))
  random.shuffle(data)
  num = int(len(data) * 0.8)
  train = data[: num]
  test = data[num:]
  return train, test
```

```python
# step1: 处理训练数据
click_path = "./data/click_log.csv"
redis_url = "127.0.0.1:6379/1"
pool = redis.from_url(redis_url)
# 构建正负样本
train, test = get_fm_recall_data(click_path, pool)

train_tohash = "./data/train_tohash"
test_tohash = "./data/test_tohash"
tohash(train, train_tohash)
tohash(test , test_tohash)

train_tfrecord_path = "./data/train"
val_tfrecord_path = "./data/val"
totfrecords(train_tohash, train_tfrecord_path)
totfrecords(test_tohash, val_tfrecord_path)
```

```python
# step2: 输入层跟之前matrix_cf的input_fn函数，这里不赘述了
```

```python
# step3: fm.py 模型讲解
weight_dim = 17 #前16维是隐向量，最后一维是w权重

learning_rate = 0.01
f_nums = 4 # 特征值是4个

def fm_fn(inputs, is_test):
  # 取特征和y值，feature为 user_id 和 movie_id
  weight_ = tf.reshape(
  			inputs["feature_embedding"],
  			shape = [-1, f_nums, weight_dim]) # [batch, f_nums, weight_dim]
  # split linear weight and cross weight
  weight_ = tf.split(weight_, num_or_size_splits = [weight_dim - 1, 1], axis = 2)
  
  # linear part
  bias_part = tf.get_variable(
  				"bias", [1, ],
  				initializer = tf.zeros_initializer()) #1*1 权重w
  
  # 求LR模型的值 w0 + w1x1 + w2x2 + ...
  linear_part = tf.nn.bias_add(
  		tf.reduce_sum(weight_[1], axis = 1), #对w值进行求和
  		bias_part) # batch*1 # 把w0加上
 	
  # cross part
  # cross sub part : sum_square part
  summed_square = tf.square(tf.reduce_sum(weight_[0], axis = 1)) # batch * embeddd
  square_summed = tf.reduce_sum(tf.square(weight_[0], axis = 1)) # batch * embeddd
  cross_part = 0.5 * tf.reduce_sum(
  				tf.subtract(summed_square, square_summed),
  				axis = 1, keepdims = True) # batch * 1
  
  out_ = linear_part + cross_part
  out_tmp = tf.sigmoid(out_)
  
  if is_test:
    tf.add_to_collections("input_tensor" , weight_)
    tf.add_to_collections("output_tensor", out_tmp)
  # 损失函数 loss label = inputs["label"]
  loss_ = tf.reduce_mean(
  		tf.nn.sigmoid_cross_entropy_with_logits(
      	logits = out_, labels = inputs["label"]))
  
  out_dic = {
    "loss": loss_,
    "ground_truth": inputs["label"][:, 0],
    "prediction": out_[:, 0]
  }
  
  return out_dic
```

```python
def setup_graph(inputs, is_test = False):
  result = {}
  with tf.variable_scope("net_graph", reuse = is_test):
    # init graph
    net_out_dic = fm_fn(inputs, is_test)
    
    loss = net_out_dic["loss"]
    result["out"] = net_out_dic
    
    if is_test:
      return result
    
    # SGD
    emb_grad = tf.gradients(
    	loss, [inputs["feature_embedding"]], name = "feature_embedding")[0]
    
    result["feature_new_embedding"] = inputs["feature_embedding"] - learning_rate * emb_grad
    
    result["feature_embedding"] = inputs["feature_embedding"]
    result["feature"] = inputs["feature"]
```

```python
# 进行正式训练
from ps import PS
from inputs import InputFn
from auc import AUCUtil

# 参数服务
local_ps = PS(weight_dim)

# 评估函数
train_metric = AUCUtil()
test_metric = AUCUtil()

# 数据输入
train_file = "./data/train"
test_file = "./data/val"
saved_embedding = "./data/saved_fm_weight"

batch_size = 64
inputs = InputFn(local_ps, f_nums, batch_size)
train_iterator, train_inputs = inputs.input_fn(train_file, is_test = False)
train_dic = setup_graph(train_inputs, is_test = False)

test_iterator, test_inputs = inputs.input_fn(test_file, is_test = True)
test_dic = setup_graph(test_inputs, is_test = True)

# 训练参数
max_steps = 10000
train_log_iter = 1000
test_show_step = 1000
last_test_auc = 0.5

def train():
  _step = 0
  # 建立session，进行训练
  with tf.Session() as session:
    # init global & local variables
    session.run([tf.global_variables_initializer(),
                tf.local_variables_initializer()])
    # 开始训练
    session.run(train_iterator.initializer)
    while _step > max_steps:
      feature_old_embedding, feature_new_embedding, keys, out = session.run(
      	[train_dic["feature_embedding"],
         train_dic["feature_new_embedding"],
         train_dic["feature"],
         train_dic["out"]])
    train_metric.add(
      out["loss"],
      out["ground_truth"],
      out["prediction"]
    )
    local_ps.push(keys, feature_new_embedding)
    _step += 1
    
    # 每训练多少个batch的数据，就打印一次训练的这些batch的auc等信息
    if _step % train_log_iter == 0:
      print("Train at setp : %d : %s", _step, train_metric.calc_str())
      train_metric.reset()
    if _step % test_show_step == 0:
      # 打印测试集的auc结果
      valid_step(session, test_iterator, test_dic)

def valid_step(session, test_iterator, test_dic):
  test_metric.reset()
  session.run(test_iterator.initializer)
  global last_test_auc
  while True:
    try:
      out = session.run(test_dic["out"])
      test_metric.add(
        out["loss"],
        out["ground_truth"],
        out["prediction"])
    except tf.errors.OutOfRangeError:
      print("Test at setp %s", test_metric.calc_str())
      if test_metric.calc()["auc"] > last_test_auc:
        last_test_auc = test_metric.calc()["auc"]
        local_ps.save(saved_embedding)
      break
```

```python
# 根据saved_fm_weight构建i2i召回

# 读取saved_fm_weight文件
def read_embedding_file(file):
  dic = dict()
  with open(file) as f:
    for line in f:
      tmp = line.split("\t")
      embedding = [float(_) for _ in tmp[1].split(",")][:-1]
      dic[tmp[0]] = embedding
  return dic 

# 将id还原
def get_hash2id(data_path):
  ds = pd.read_csv(data_path)
  ds = ds[['user_id', 'article_id', 'environment', 'region']]
  users = list(ds['user_id']).unique()
  items = list(ds['article_id']).unique()
  environment = list(ds['environment']).unique()
  region = list(ds['region']).unique()
  
  users_dict= {str(bkdr2hash64("user_id=" + str(u))): int(u) for u in users}
  items_dict= {str(bkdr2hash64("article_id=" + str(i))): int(i) for i in items}
  environment_dict = {str(bkdr2hash64("environment=" + str(i))): int(i) for i in environment}
  region_dict= {str(bkdr2hash64("region=" + str(i))): int(i) for i in region}
  return users_dict, items_dict, environment_dict, region_dict

def split_user_item(embedding_file, train_file):
  user_dict, items_dict, env_dict, region_dict = get_hash2id(train_file)
  embedding_dict = read_embedding_file(embedding_file)
  
  item_embedding = {}
  user_embedding = {}
  
  for k, v in embedding_dict.items():
    m_id = item_dict.get(k, None)
    if m_id is not None:
      item_embedding[m_id] = v
    user_id = user_dict.get(k, None)
    if user_id is not None:
      user_embedding("user_id" + str(user_id)) = v
    env_ = env_dict.get(k, None)
    if end_ is not None:
      user_embedding("env=" + str(env_)) = v
    region_ = region_dict.get(k, None)
    if region_ is not None:
      user_embedding("region=" + str(region_)) = v
  print('item_embedding size: ' len(item_embedding))
  print('user_embedding size: ' len(user_embedding))
  return item_embddding, user_embedding

# 用于i2i模式
def embedding_sim(item_embedding, cut_off = 20):
  # 向量进行正则化
  item_idx_2_rawid_dict = {}
  item_emb_np = []
  for i, (k, v) in enumerate(item_embedding.items()):
    item_idx_2_rawid_dict[i] = k
    item_emb_np.append(v)
    
  item_emb_np = np.asarray(item_emb_np)
  item_emb_np = item_emb_np / np.linalg.norm(
  	item_emb_np, axis = 1, keepdims = True)
  
  # 建立faiss/BallTree索引
  item_tree = neighbors.BallTree(item_emb_np, leaf_size = 40)
  
  # 相似度查询，给每个索引位置上的向量返回top K个item以及相似度
  sim, idx = item_tree.query(item_emb_np. cut_off)
  
  # 将向量索引的结果保存为原始id的对应关系
  item_emb_sim_dict = {}
  for target_idx, sim_value_list, rele_idx_list in tqdm(
  			zip(range(len(item_emb_np)), sim, idx)):
    target_raw_id = item_idx_2_rawid_dict[target_idx]
    sim_tmp = {}
    for rele_idx, sim_value in zip(rele_idx_list[1:], sim_value_list[1:]):
      rele_raw_id = item_idx2_rawid_dict[rele_idx]
      sim_tmp[rele_raw_id] = sim_value
    item_emb_sim_dict[target_raw_id] = sorted(
    	sim_tmp.items(), key = lambda _: _[1], reverse = True)[: cut_off]
    
  # 保存i2i相似矩阵
  item_simi_tuple = [(_, json.dumps(v)) for _, v in item_emb_sim_dict.items()]
  save_redis(item_simi_tuple, db = 6)
  
# 测试
# data_path = "./data"
# embedding_file = data_path + "saved_fm_weight"
# train_file = data_path + "click_log.csv"
# item_embdding, user_embedding = split_user_item(embedding_file)
# write_embedding(item_embedding, data_path + "fm_articles_emb")
# write_embedding(user_embedding, data_path + "fm_user_emb")
# embedding_sim(item_embedding, 20)
```

```python
# 文章隐向量
embedding_file = "./data/fm_articles_emb"
save_redis(read_embedding_file(embedding_file), db = 7)
```

```python
# 用户隐向量
embedding_file = "./data/fm_user_emb"
save_redis(read_embedding_file(embedding_file), db = 8)
```



## 3. 资讯召回推荐线上模拟



![online-process](/Users/menglingfeng/Documents/GitHub/RecSys/images/online-process.jpg)



### 3.1 向量服务实现

```python
class VectorServer:
  def __init__(self, pool):
    
    self.pool = pool
    self.keys_index =[]
    self.vector_matrix =[]
    
    keys = self.pool.keys()
    pipe = self.pool.pipeline()
    key_list = []
    s = 0
    for key in keys:
      key_list.append(key)
      pipe.get(key)
      if s < 10000:
        s += 1
        else:
          for k, v in zip(key_list, pipe.execute()):
            vec = json.loads(v)
            self.keys_index.append(int(k))
            self.vector_matrix.append(vec)
    for k, v in zip(key_list, pipe.execute()):
      vec = json.loads(v)
      self.keys_index.append(int (k))
      self.vector_matrix.append(vec)
      
    item_emb_np = np.asarray(self.vector_matrix)
    item_emb_np = item_emb_np / np.lialg.norm(
    	item_emb_np, axis = 1, keepdims = True)
    # 建立faiss/BallTree索引
    self.item_tree = neighbors.BallTree(item_emb_np, leaf_size = 40)
  
  # items:[vector, vector, vector] -> n*embedding的矩阵
  def get_sim_item(self, items, cut_off):
    sim, idx = self.item_tree.query(items, cut_off)
    
    items_result = []
    for i in range(len(sim)):
      items = [self.keys_index[_] for _ in idx[i]]
      item_sim_score = dict(zip(items, sim[i]))
      item_sim_score = sorted(
      	item_sim_score.items(), key = lambda _: _[1], reverse = True)[: cut_off]
      items_result.append(item_sim_score)
    return items_result
```

### 3.2 Recall Server

```python
class RecallServer:
  def __init__(self):
    # redis
    self.user_feature_pool 			= redis_from_url("127.0.0.1:6379/1")
    self.item_feature_pool 			= redis_from_url("127.0.0.1:6379/2")
    self.matrixcf_i2i_pool 			= redis_from_url("127.0.0.1:6379/3")
    self.itemcf_i2i_pool   			= redis_from_url("127.0.0.1:6379/4")
    self.usercf.u2u_pool   			= redis_from_url("127.0.0.1:6379/5")
    self.fm_i2i_pool       			= redis_from_url("127.0.0.1:6379/6")
    self.fm_item_embedding_pool = redis_from_url("127.0.0.1:6379/7")
    self.fm_user_embedding_pool = redis_from_url("127.0.0.1:6379/8")
    
    # 定义一个缓存item信息的缓存区
    self.user_info = {}
    self.item_info = {}
    
    # 当前用户
    self.current_user_feature = {}
    
    # 向量服务（为了构建u2i）
    self.vectorserver = VectorServer(self.fm_item_embedding_pool)
    
  def set_user_info(self):
    v = user_info['user_id']
    self.current_user_feature = self.user_info.get(str(u), None)
    if self.current_user_feature is None:
      self.current_user_feature = json.loads(self.user_feature_pool.get(str(u)))
      self.user_info[str(u)] = self.current_user_feature
  
  # u2i2i
  def get_item_cf_recommeng_result(self, recall_num = 30):
    item_rank = {}
    hists = self.current_user_feature['hists']
    for loc, (i, _) in enumerate(hists):
      item_sim_ = json.loads(slef.itemcf.i2i_pool.get(str(i)))
      
      for j, wij in item_sim_:
        if j in hists: # 去重
          continue
          item_i_info = self.item_info.get(i, None)
          if item_i_info is None:
            item_i_info = json_loads(self.item_feature_pool.get(str(i)))
          self.item_info[i] = item_i_info
          item_j_info = self.item_info.get(j, None)
          if item_j_info is None:
            item_j_info = json_loads(self.item_feature_pool.get(str(j)))
          self.item_info[j] = item_j_info
    			
          # 两篇文章类别权重，其中类别相同权重越大
          type_weight = 1.0 if item_i_info['category_id'] == item_j_info['category_id'] else 0.7
          
          # 时间权重，点击时间相近，权重越大
          created_time_weight = np.exp(
          	0.7 ** np.abs(item_i_info['created_at_ts'] - item_j_info['created_at_ts']))
          
          # 相似文章和历史点击文章序列中历史文章所在的位置权重
          loc_weight = (0.9 ** (len(hists) - loc))
          item_rank.setdefault(j, 0)
          item_rank[j] += loc_weight * type_weight * created_time_weight * wij
    
    item_rank = sorted(item_rank.items(), key = lambda x: x[1], reverse = True)[: recall_num]
    
    return item_rank
  
  # u2u2i
  def get_user_cf_recommend_result(self, recall_num = 30):
    # ... 
  # u2i2i 
  def get_matrix_cf_recommend_result(self, recall_num = 30):
  	# ...
  # u2i2i 
  def get_fm_i2i_recommend_result(self, recall_num = 30):
  	# ...这里不再叙述
  
  # u2i
  def fm_u2i_recommend_result(self, recall_num = 30):
    user_id = "user_id=" + str(self.current_user_feature['user_id'])
    env = "env=" + str(self.current_user_feature['environment'])
    region = "region=" + str(self.current_user_feature['region'])
    
    # mget是批量获取
    emb = self.fm_user_feature_embedding_pool.mget([user_id, env, region])
    emb = [json.loads(_) for _ in emb]
    emb = np.sum(np.asarray(emb), axis = 0, keepdims = True)
    items_rec_ = self.vectorserver.get_sim_item(emb, recall_num)
    item_rank = items_rec_[0]
    return item_rank
  
  def merge_recall_result(self, item_ranks):
    item_rec = {}
    for item_rank, weight in item_ranks:
      tmp = [_[1] for _ in item_rank]
      max_value = max(tmp)
      min_value = min(tmp)
      for i, w in item_rank:
        item_rec.setdefault(i, 0)
        item_rec[i] += weight * (w - min_value) / (
        	max_value - min_value)
        
    return item_rec
    
  def main():
    rs = RecallServer()
    rec_user = {'user_id' : 19000}
    rs.set_user_info(rec_user)
    itemcf_item_rank = rs.get_item_cf_recommend_result()
    usercf_item_rank = rs.get_user_cf_recommend_result()
    matrixcf_item_rank = rs.get_matrix_cf_recommend_result()
    
    item_rec = rs.merge_recall_result(
      [(itemcf_item_rank, 1.0),
       (usercf_item_rank, 1.0),
       (matrixcf_item_rank, 1.0)
      ]
    )
```

